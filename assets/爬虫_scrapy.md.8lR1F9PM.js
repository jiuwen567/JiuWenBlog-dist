import{_ as s,c as i,o as a,a6 as n}from"./chunks/framework.DkFL-jqo.js";const g=JSON.parse('{"title":"","description":"","frontmatter":{},"headers":[],"relativePath":"爬虫/scrapy.md","filePath":"爬虫/scrapy.md"}'),p={name:"爬虫/scrapy.md"},l=n(`<h3 id="简介" tabindex="-1">简介 <a class="header-anchor" href="#简介" aria-label="Permalink to &quot;简介&quot;">​</a></h3><p>什么是框架？</p><p>所谓的框，其实说白了就是一个【项目的半成品】，该项目的半成品需要被集成了各种功能且具有较强的通用性。</p><p>Scrapy是一个为了爬取网站数据，提取结构性数据而编写的应用框架，非常出名，非常强悍。所谓的框架就是一个已经被集成了各种功能（高性能异步下载，队列，分布式，解析，持久化等）的具有很强通用性的项目模板。对于框架的学习，重点是要学习其框架的特性、各个功能的用法即可。</p><p>初期如何学习框架？</p><p>只需要学习框架集成好的各种功能的用法即可！前期切勿钻研框架的源码！</p><p>Scrapy到目前为止依然是这个星球上最流行的爬虫框架. 摘一下官方给出对scrapy的介绍</p><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>An open source and collaborative framework for extracting the data you need from websites.</span></span>
<span class="line"><span></span></span>
<span class="line"><span>In a fast, simple, yet extensible way.</span></span></code></pre></div><p>​ scrapy的特点: 速度快, 简单, 可扩展性强.</p><p>​ scrapy的官方文档: <a href="https://docs.scrapy.org/en/latest/" target="_blank" rel="noreferrer">https://docs.scrapy.org/en/latest/</a></p><h3 id="scrapy工作流程-重点" tabindex="-1">scrapy工作流程(重点) <a class="header-anchor" href="#scrapy工作流程-重点" aria-label="Permalink to &quot;scrapy工作流程(重点)&quot;">​</a></h3><p>​ 之前我们所编写的爬虫的逻辑:</p><p><img src="https://typora5672.oss-cn-chengdu.aliyuncs.com/temp/image-20210803105808636.png" alt="image-20210803105808636"></p><p>​ scrapy的工作流程:</p><p><img src="https://typora5672.oss-cn-chengdu.aliyuncs.com/temp/image-20210803113438252.png" alt="image-20210803113438252"></p><p>整个工作流程,</p><ol><li><p>爬虫中起始的url构造成request对象, 并传递给调度器.</p></li><li><p><code>引擎</code>从<code>调度器</code>中获取到request对象. 然后交给<code>下载器</code></p></li><li><p>由<code>下载器</code>来获取到页面源代码, 并封装成response对象. 并回馈给<code>引擎</code></p></li><li><p><code>引擎</code>将获取到的response对象传递给<code>spider</code>, 由<code>spider</code>对数据进行解析(parse). 并回馈给<code>引擎</code></p></li><li><p><code>引擎</code>将数据传递给pipeline进行数据持久化保存或进一步的数据处理.</p></li><li><p>在此期间如果spider中提取到的并不是数据. 而是子页面url. 可以进一步提交给调度器, 进而重复<code>步骤2</code>的过程</p></li></ol><p>上述过程中一直在重复着几个东西,</p><ol><li><p>引擎(engine)</p><p>scrapy的核心, 所有模块的衔接, 数据流程梳理.</p></li><li><p>调度器(scheduler)</p><p>本质上这东西可以看成是一个队列. 里面存放着一堆我们即将要发送的请求. 可以看成是一个url的容器. 它决定了下一步要去爬取哪一个url. 通常我们在这里可以对url进行去重操作.</p></li><li><p>下载器(downloader)</p><p>它的本质就是用来发动请求的一个模块. 小白们完全可以把它理解成是一个get_page_source()的功能. 只不过这货返回的是一个response对象.</p></li><li><p>爬虫(spider)</p><p>这是我们要写的第一个部分的内容, 负责解析下载器返回的response对象.从中提取到我们需要的数据.</p></li><li><p>管道(pipeline)</p><p>这是我们要写的第二个部分的内容, 主要负责数据的存储和各种持久化操作.</p></li></ol><p>经过上述的介绍来看, scrapy其实就是把我们平时写的爬虫进行了四分五裂式的改造. 对每个功能进行了单独的封装, 并且, 各个模块之间互相的不做依赖. 一切都由引擎进行调配. 这种思想希望你能知道--解耦. 让模块与模块之间的关联性更加的松散. 这样我们如果希望替换某一模块的时候会非常的容易. 对其他模块也不会产生任何的影响.</p><p>到目前为止, 我们对scrapy暂时了解这么多就够了. 后面会继续在这个图上进一步展开.</p><h3 id="安装" tabindex="-1">安装 <a class="header-anchor" href="#安装" aria-label="Permalink to &quot;安装&quot;">​</a></h3><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>Linux/mac系统：</span></span>
<span class="line"><span>      pip install scrapy（任意目录下）</span></span>
<span class="line"><span></span></span>
<span class="line"><span>Windows系统：</span></span>
<span class="line"><span></span></span>
<span class="line"><span>      a. pip install wheel（任意目录下）</span></span>
<span class="line"><span></span></span>
<span class="line"><span>      b. 下载twisted文件，下载网址如下： http://www.lfd.uci.edu/~gohlke/pythonlibs/#twisted</span></span>
<span class="line"><span></span></span>
<span class="line"><span>      c. 终端进入下载目录，执行 pip install Twisted‑17.1.0‑cp35‑cp35m‑win_amd64.whl</span></span>
<span class="line"><span>      注意：如果该步骤安装出错，则换一个版本的whl文件即可</span></span>
<span class="line"><span></span></span>
<span class="line"><span>      d. pip install pywin32（任意目录下）</span></span>
<span class="line"><span></span></span>
<span class="line"><span>      e. pip install scrapy（任意目录下）</span></span>
<span class="line"><span>      </span></span>
<span class="line"><span>如果安装好后，在终端中录入scrapy指令按下回车，如果没有提示找不到该指令，则表示安装成功</span></span></code></pre></div><p>==下载Visual Studio也行==</p><h3 id="基本使用" tabindex="-1">基本使用 <a class="header-anchor" href="#基本使用" aria-label="Permalink to &quot;基本使用&quot;">​</a></h3><ul><li><p>创建项目</p><ul><li><p>scrapy startproject 项目名称</p></li><li><p>项目的目录结构：</p><ul><li><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>firstBlood   # 项目所在文件夹, 建议用pycharm打开该文件夹</span></span>
<span class="line"><span>    ├── firstBlood  		# 项目跟目录</span></span>
<span class="line"><span>    │   ├── __init__.py</span></span>
<span class="line"><span>    │   ├── items.py  		# 封装数据的格式</span></span>
<span class="line"><span>    │   ├── middlewares.py  # 所有中间件</span></span>
<span class="line"><span>    │   ├── pipelines.py	# 所有的管道</span></span>
<span class="line"><span>    │   ├── settings.py		# 爬虫配置信息</span></span>
<span class="line"><span>    │   └── spiders			# 爬虫文件夹, 稍后里面会写入爬虫代码</span></span>
<span class="line"><span>    │       └── __init__.py</span></span>
<span class="line"><span>    └── scrapy.cfg			# scrapy项目配置信息,不要删它,别动它,善待它.</span></span></code></pre></div></li></ul></li></ul></li><li><p>创建爬虫爬虫文件：</p><ul><li>cd project_name（进入项目目录）</li><li>scrapy genspider 爬虫文件的名称（自定义一个名字即可） 起始url <ul><li>（例如：scrapy genspider first www.xxx.com）</li></ul></li><li>创建成功后，会在爬虫文件夹下生成一个py的爬虫文件</li></ul></li><li><p>编写爬虫文件</p><ul><li><p>理解爬虫文件的不同组成部分</p></li><li><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> scrapy</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> FirstSpider</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">scrapy</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Spider</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #爬虫名称：爬虫文件唯一标识：可以使用该变量的值来定位到唯一的一个爬虫文件</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;first&#39;</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"> #无需改动</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #允许的域名：scrapy只可以发起百度域名下的网络请求</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # allowed_domains = [&#39;www.baidu.com&#39;]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #起始的url列表：列表中存放的url可以被scrapy发起get请求</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    start_urls </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;https://www.baidu.com/&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;https://www.sogou.com&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #专门用作于数据解析</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #参数response：就是请求之后对应的响应对象</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #parse的调用次数，取决于start_urls列表元素的个数</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> parse</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, response):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;响应对象为：&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,response)</span></span></code></pre></div></li></ul></li><li><p>配置文件修改:settings.py</p><ul><li>不遵从robots协议：ROBOTSTXT_OBEY = False</li><li>指定输出日志的类型：LOG_LEVEL = &#39;ERROR&#39;</li><li>指定UA：USER_AGENT = &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36&#39;</li></ul></li><li><p>运行项目</p><ul><li><div class="language- vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang"></span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span>scrapy crawl 爬虫名称 ：该种执行形式会显示执行的日志信息（推荐）</span></span>
<span class="line"><span>scrapy crawl 爬虫名称 --nolog：该种执行形式不会显示执行的日志信息（一般不用）</span></span></code></pre></div></li></ul></li></ul><h3 id="数据解析" tabindex="-1">数据解析 <a class="header-anchor" href="#数据解析" aria-label="Permalink to &quot;数据解析&quot;">​</a></h3><ul><li><p>注意，如果终端还在第一个项目的文件夹中，则需要在终端中执行cd ../返回到上级目录，在去新建另一个项目。</p></li><li><p>新建数据解析项目：</p><ul><li>创建工程：scrapy startproject 项目名称</li><li>cd 项目名称</li><li>创建爬虫文件：scrapy genspider 爬虫文件名 www.xxx.com</li></ul></li><li><p>配置文件的修改：settings.py</p><ul><li>不遵从robots协议：ROBOTSTXT_OBEY = False</li><li>指定输出日志的类型：LOG_LEVEL = &#39;ERROR&#39;</li><li>指定UA：USER_AGENT = &#39;Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/98.0.4758.109 Safari/537.36&#39;</li></ul></li><li><p>编写爬虫文件：spiders/duanzi.py</p><ul><li><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> scrapy</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> DuanziSpider</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">scrapy</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Spider</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;duanzi&#39;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # allowed_domains = [&#39;www.xxx.com&#39;]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #对首页进行网络请求</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #scrapy会对列表中的url发起get请求</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    start_urls </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;https://ishuo.cn/duanzi&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> parse</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, response):</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        #如何获取响应数据</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        #调用xpath方法对响应数据进行xpath形式的数据解析</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        li_list </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> response.xpath(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;//*[@id=&quot;list&quot;]/ul/li&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li_list:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # content = li.xpath(&#39;./div[1]/text()&#39;)[0]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # title = li.xpath(&#39;./div[2]/a/text()&#39;)[0]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # #&lt;Selector xpath=&#39;./div[2]/a/text()&#39; data=&#39;一年奔波，尘缘遇了谁&#39;&gt;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # print(title)#selector的对象，且我们想要的字符串内容存在于该对象的data参数里</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            #解析方案1：</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # title = li.xpath(&#39;./div[2]/a/text()&#39;)[0]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # content = li.xpath(&#39;./div[1]/text()&#39;)[0]</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # #extract()可以将selector对象中data参数的值取出</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # print(title.extract())</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            # print(content.extract())</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            #解析方案2：</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            #title和content为列表，列表只要一个列表元素</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            title </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li.xpath(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;./div[2]/a/text()&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            content </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li.xpath(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;./div[1]/text()&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            #extract_first()可以将列表中第0个列表元素表示的selector对象中data的参数值取出</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(title.extract_first())</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">            print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(content.extract_first())</span></span></code></pre></div></li></ul></li></ul><h3 id="持久化存储" tabindex="-1">持久化存储 <a class="header-anchor" href="#持久化存储" aria-label="Permalink to &quot;持久化存储&quot;">​</a></h3><p>两种方案：</p><ul><li>基于终端指令的持久化存储</li><li>基于管道的持久化存储（推荐）</li></ul><h4 id="基于终端指令的持久化存储" tabindex="-1">基于终端指令的持久化存储 <a class="header-anchor" href="#基于终端指令的持久化存储" aria-label="Permalink to &quot;基于终端指令的持久化存储&quot;">​</a></h4><ul><li><p>只可以将parse方法的返回值存储到指定后缀的文本文件中。</p></li><li><p>编码流程：</p><ul><li><p>在爬虫文件中，将爬取到的数据全部封装到parse方法的返回值中</p><ul><li><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> scrapy</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> DemoSpider</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">scrapy</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Spider</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    name </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;"> &#39;demo&#39;</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # allowed_domains = [&#39;www.xxx.com&#39;]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    start_urls </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> [</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;https://ishuo.cn/duanzi&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> parse</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, response):</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 如何获取响应数据</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 调用xpath方法对响应数据进行xpath形式的数据解析</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        li_list </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> response.xpath(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;//*[@id=&quot;list&quot;]/ul/li&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        all_data </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> []</span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#爬取到的数据全部都存储到了该列表中</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li_list:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            title </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li.xpath(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;./div[2]/a/text()&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).extract_first()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            content </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li.xpath(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;./div[1]/text()&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).extract_first()</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            #将段子标题和内容封装成parse方法的返回</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            dic </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> {</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                &#39;title&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:title,</span></span>
<span class="line"><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">                &#39;content&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:content</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            }</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            all_data.append(dic)</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> all_data</span></span></code></pre></div></li></ul></li><li><p>将parse方法的返回值存储到指定后缀的文本文件中:</p><ul><li>scrapy crawl 爬虫文件名称 -o duanzi.csv</li></ul></li></ul></li><li><p>总结：</p><ul><li>优点：简单，便捷</li><li>缺点：局限性强 <ul><li>只可以将数据存储到文本文件无法写入数据库</li><li>存储数据文件后缀是指定好的，通常使用.csv</li><li>需要将存储的数据封装到parse方法的返回值中</li></ul></li></ul></li></ul><h4 id="基于管道实现持久化存储" tabindex="-1">基于管道实现持久化存储 <a class="header-anchor" href="#基于管道实现持久化存储" aria-label="Permalink to &quot;基于管道实现持久化存储&quot;">​</a></h4><p>优点：极大程度的提升数据存储的效率</p><p>缺点：编码流程较多</p><h5 id="编码流程" tabindex="-1">编码流程 <a class="header-anchor" href="#编码流程" aria-label="Permalink to &quot;编码流程&quot;">​</a></h5><p>1.在爬虫文件中进行数据解析</p><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> parse</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, response):</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 如何获取响应数据</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">  # 调用xpath方法对响应数据进行xpath形式的数据解析</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  li_list </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> response.xpath(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;//*[@id=&quot;list&quot;]/ul/li&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">  all_data </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> []  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 爬取到的数据全部都存储到了该列表中</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">  for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li_list:f</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    title </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li.xpath(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;./div[2]/a/text()&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).extract_first()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    content </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li.xpath(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;./div[1]/text()&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).extract_first()</span></span></code></pre></div><p>2.将解析到的数据封装到Item类型的对象中</p><ul><li><p>2.1 在items.py文件中定义相关的字段</p><ul><li><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> SavedataproItem</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">scrapy</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;">Item</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">):</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # define the fields for your item here like:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    # name = scrapy.Field()</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #爬取的字段有哪些，这里就需要定义哪些变量存储爬取到的字段</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    title </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> scrapy.Field()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    content </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> scrapy.Field()</span></span></code></pre></div></li></ul></li><li><p>2.2 在爬虫文件中引入Item类，实例化item对象，将解析到的数据存储到item对象中</p><ul><li><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> parse</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, response):</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    		from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> items </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SavedataproItem </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#导入item类</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 如何获取响应数据</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # 调用xpath方法对响应数据进行xpath形式的数据解析</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        li_list </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> response.xpath(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;//*[@id=&quot;list&quot;]/ul/li&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        all_data </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> []  </span><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># 爬取到的数据全部都存储到了该列表中</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        for</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">in</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li_list:</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            title </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li.xpath(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;./div[2]/a/text()&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).extract_first()</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            content </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> li.xpath(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;./div[1]/text()&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">).extract_first()</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            #实例化一个item类型的对象</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            item </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> SavedataproItem()</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">            #通过中括号的方式访问item对象中的两个成员，且将解析到的两个字段赋值给item对象的两个成员即可</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            item[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;title&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> title</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">            item[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;content&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">] </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> content</span></span></code></pre></div></li></ul></li></ul><p>3.将item对象提交给管道</p><ul><li><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#将存储好数据的item对象提交给管道</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">yield</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> item</span></span></code></pre></div></li></ul><p>==需要开启管道==</p><p><img src="https://typora567.oss-cn-chengdu.aliyuncs.com/temp_picture/image-20230408235322158.png" alt="image-20230408235322158"></p><p>4.在管道中接收item类型对象(pipelines.py就是管道文件)</p><ul><li><p>管道只可以接收item类型的对象，不可以接收其他类型对象</p></li><li><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> SavedataproPipeline</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #process_item用来接收爬虫文件传递过来的item对象</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #item参数，就是管道接收到的item类型对象</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> process_item</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, item, spider):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(item)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> item</span></span></code></pre></div></li></ul><p>5.在管道中对接收到的数据进行任意形式的持久化存储操作</p><ul><li><p>可以存储到文件中也可以存储到数据库中</p></li><li><div class="language-python vp-adaptive-theme"><button title="Copy Code" class="copy"></button><span class="lang">python</span><pre class="shiki shiki-themes github-light github-dark vp-code" tabindex="0"><code><span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Define your item pipelines here</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">#</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># Don&#39;t forget to add your pipeline to the ITEM_PIPELINES setting</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># See: https://docs.scrapy.org/en/latest/topics/item-pipeline.html</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;"># useful for handling different item types with a single interface</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">from</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> itemadapter </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">import</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> ItemAdapter</span></span>
<span class="line"></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">class</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> SavedataproPipeline</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">:</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #重写父类的方法</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">    fp </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> None</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> open_spider</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self,spider):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;我是open_spider方法，我在项目开始运行环节，只会被执行一次！&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.fp </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;"> open</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;duanzi.txt&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;w&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">,</span><span style="--shiki-light:#E36209;--shiki-dark:#FFAB70;">encoding</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;utf-8&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #process_item用来接收爬虫文件传递过来的item对象</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #item参数，就是管道接收到的item类型对象</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">    #process_item方法调用的次数取决于爬虫文件给其提交item的次数</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> process_item</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self, item, spider):</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        #item类型的对象其实就是一个字典</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        # print(item)</span></span>
<span class="line"><span style="--shiki-light:#6A737D;--shiki-dark:#6A737D;">        #将item字典中的标题和内容获取</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        title </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> item[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;title&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">        content </span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">=</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> item[</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;content&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">]</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.fp.write(title</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;:&#39;</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">content</span><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">+</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;</span><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">\\n</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(title,</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;:爬取保存成功！&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">        return</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;"> item</span></span>
<span class="line"></span>
<span class="line"><span style="--shiki-light:#D73A49;--shiki-dark:#F97583;">    def</span><span style="--shiki-light:#6F42C1;--shiki-dark:#B392F0;"> close_spider</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(self,spider):</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        print</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">(</span><span style="--shiki-light:#032F62;--shiki-dark:#9ECBFF;">&#39;在爬虫结束的时候会被执行一次！&#39;</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">)</span></span>
<span class="line"><span style="--shiki-light:#005CC5;--shiki-dark:#79B8FF;">        self</span><span style="--shiki-light:#24292E;--shiki-dark:#E1E4E8;">.fp.close()</span></span></code></pre></div></li></ul><p>6.在配置文件中开启管道机制</p><ul><li>注意：默认情况下，管道机制是没有被开启的，需要在配置文件中手动开启</li><li>在setting.py中把ITEM_PIPELINES解除注释就表示开启了管道机制</li></ul><h3 id="总结" tabindex="-1">总结 <a class="header-anchor" href="#总结" aria-label="Permalink to &quot;总结&quot;">​</a></h3><p>scrapy框架的使用流程:</p><ol><li>创建爬虫项目. <code>scrapy startproject xxx </code></li><li>进入项目目录. <code>cd xxx </code></li><li>创建爬虫 <code>scrapy genspider 名称 抓取域</code></li><li>编写<code>item.py</code> 文件, 定义好数据item</li><li>修改spider中的parse方法. 对返回的响应response对象进行解析. 返回item</li><li>在pipeline中对数据进行保存工作.</li><li>修改<code>settings.py</code>文件, 将pipeline设置为生效, 并设置好优先级</li><li>启动爬虫 <code>scrapy crawl 名称</code></li></ol>`,54),t=[l];function h(e,k,r,d,E,c){return a(),i("div",null,t)}const o=s(p,[["render",h]]);export{g as __pageData,o as default};
